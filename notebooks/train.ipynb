{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer.tokenizer import Tokenizer\n",
    "from huggingface_hub import login\n",
    "from trainer import TrainerConfig, Trainer\n",
    "from hf_backed_trainer import HFBackedTrainer\n",
    "from model.gpt2 import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cc3f5b0bbd43ca88c5f6be0404fefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cfg_file = \"config/news_trainer.yml\"\n",
    "model_cfg_file = \"config/news_model.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file)->TrainerConfig:\n",
    "    import yaml\n",
    "\n",
    "    with open(config_file) as f:\n",
    "        try:\n",
    "            doc = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            exit(1)\n",
    "    \n",
    "    return TrainerConfig(**doc)\n",
    "\n",
    "trainer_cfg = load_config(trainer_cfg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('habanoz/haber-gpt-v1.0', 'habanoz/eco-news-tr')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_cfg.repo_id, trainer_cfg.ds_repo_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cffa3ab3a0498d9c8bf16e4abb1b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/383k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\habanoz\\.cache\\huggingface\\hub\\models--habanoz--haber-gpt-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(trainer_cfg.repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'train' tokens: 4767677\n",
      "Number of 'validation' tokens: 535192\n"
     ]
    }
   ],
   "source": [
    "tokenizer.encode_ds_from_hub(trainer_cfg.ds_repo_id, trainer_cfg.data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 13.77M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(8192, 384)\n",
       "    (wpe): Embedding(512, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=8192, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT.from_config(model_cfg_file)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea951aebb6b34c1d93d2f2d5b7ed4c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/55.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3788b122f84fcb9b7bdcee7a4e6d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/112M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume training. Using repo habanoz/haber-gpt-v1.0\n",
      "Loading model state dict!\n",
      "Loaded model state dict!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuseyinabanozis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\wandb\\run-20240719_075333-1721342904</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/huseyinabanozis/NB-Haber-GPT-Training/runs/1721342904' target=\"_blank\">haber-gpt-v1.0</a></strong> to <a href='https://wandb.ai/huseyinabanozis/NB-Haber-GPT-Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/huseyinabanozis/NB-Haber-GPT-Training' target=\"_blank\">https://wandb.ai/huseyinabanozis/NB-Haber-GPT-Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/huseyinabanozis/NB-Haber-GPT-Training/runs/1721342904' target=\"_blank\">https://wandb.ai/huseyinabanozis/NB-Haber-GPT-Training/runs/1721342904</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 13,959,168 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\model\\gpt2.py:67: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback iter: 750, best val loss: 2.7681424617767334\n",
      "saving checkpoint to haber-gpt\n",
      "Eval iter 750: train loss 2.6788, val loss 2.7681, ETA 0:00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420bc389401b4fb8a21b8bcedd412007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/112M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 750: loss 2.9291, run_iter_time 951.00ms, fb_toks/sec 17228.10, run_fb_toks/sec 17228.10\n",
      "Completed haber-gpt\\pytorch_model.bin\n",
      "iter 760: loss 2.6150, run_iter_time 953.81ms, fb_toks/sec 16734.78, run_fb_toks/sec 17178.77\n",
      "iter 770: loss 2.8479, run_iter_time 961.78ms, fb_toks/sec 15852.74, run_fb_toks/sec 17046.17\n",
      "iter 780: loss 2.7325, run_iter_time 977.10ms, fb_toks/sec 14694.13, run_fb_toks/sec 16810.96\n",
      "iter 790: loss 2.6428, run_iter_time 987.65ms, fb_toks/sec 15134.49, run_fb_toks/sec 16643.31\n",
      "Completed haber-gpt\\trainer_state.bin\n",
      "iter 800: loss 2.6024, run_iter_time 996.09ms, fb_toks/sec 15282.67, run_fb_toks/sec 16507.25\n",
      "iter 810: loss 2.7854, run_iter_time 990.08ms, fb_toks/sec 17504.89, run_fb_toks/sec 16607.01\n",
      "iter 820: loss 2.5581, run_iter_time 1005.37ms, fb_toks/sec 14334.04, run_fb_toks/sec 16379.72\n",
      "iter 830: loss 2.5355, run_iter_time 1022.34ms, fb_toks/sec 13943.27, run_fb_toks/sec 16136.07\n",
      "iter 840: loss 2.7172, run_iter_time 1037.16ms, fb_toks/sec 13996.58, run_fb_toks/sec 15922.12\n",
      "iter 850: loss 2.7505, run_iter_time 1041.40ms, fb_toks/sec 15176.52, run_fb_toks/sec 15847.56\n",
      "iter 860: loss 2.4308, run_iter_time 1050.72ms, fb_toks/sec 14440.95, run_fb_toks/sec 15706.90\n",
      "iter 870: loss 2.6350, run_iter_time 1057.05ms, fb_toks/sec 14707.22, run_fb_toks/sec 15606.93\n",
      "iter 880: loss 2.7076, run_iter_time 1069.64ms, fb_toks/sec 13849.88, run_fb_toks/sec 15431.23\n",
      "iter 890: loss 2.7730, run_iter_time 1080.97ms, fb_toks/sec 13849.50, run_fb_toks/sec 15273.05\n",
      "iter 900: loss 2.9699, run_iter_time 1076.48ms, fb_toks/sec 15815.00, run_fb_toks/sec 15327.25\n",
      "iter 910: loss 2.7614, run_iter_time 1077.13ms, fb_toks/sec 15128.64, run_fb_toks/sec 15307.39\n",
      "iter 920: loss 2.4026, run_iter_time 1090.02ms, fb_toks/sec 13584.92, run_fb_toks/sec 15135.14\n",
      "iter 930: loss 2.8599, run_iter_time 1091.82ms, fb_toks/sec 14786.94, run_fb_toks/sec 15100.32\n",
      "iter 940: loss 2.3101, run_iter_time 1093.84ms, fb_toks/sec 14733.72, run_fb_toks/sec 15063.66\n",
      "iter 950: loss 2.6268, run_iter_time 1094.70ms, fb_toks/sec 14861.48, run_fb_toks/sec 15043.44\n",
      "iter 960: loss 2.8744, run_iter_time 1096.54ms, fb_toks/sec 14718.98, run_fb_toks/sec 15011.00\n",
      "iter 970: loss 2.7960, run_iter_time 1086.99ms, fb_toks/sec 16366.99, run_fb_toks/sec 15146.60\n",
      "iter 980: loss 2.6470, run_iter_time 1084.49ms, fb_toks/sec 15427.41, run_fb_toks/sec 15174.68\n",
      "iter 990: loss 2.5105, run_iter_time 1087.74ms, fb_toks/sec 14667.77, run_fb_toks/sec 15123.99\n",
      "Callback iter: 1000, best val loss: 2.646878242492676\n",
      "saving checkpoint to haber-gpt\n",
      "Eval iter 1000: train loss 2.4853, val loss 2.6469, ETA 1:58:49.197096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3697d03f99054c31aff195c387000487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/55.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c468027fac4333bd639332f3d0669a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/112M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000: loss 2.5153, run_iter_time 1095.57ms, fb_toks/sec 14050.99, run_fb_toks/sec 15016.69\n",
      "iter 1010: loss 2.5902, run_iter_time 1122.01ms, fb_toks/sec 12047.04, run_fb_toks/sec 14719.72\n",
      "iter 1020: loss 2.6978, run_iter_time 1105.71ms, fb_toks/sec 17085.04, run_fb_toks/sec 14956.26\n",
      "iter 1030: loss 2.6432, run_iter_time 1142.34ms, fb_toks/sec 11130.63, run_fb_toks/sec 14573.69\n",
      "Completed haber-gpt\\pytorch_model.bin\n",
      "iter 1040: loss 2.6090, run_iter_time 1142.30ms, fb_toks/sec 14346.67, run_fb_toks/sec 14550.99\n",
      "iter 1050: loss 2.5420, run_iter_time 1141.97ms, fb_toks/sec 14384.98, run_fb_toks/sec 14534.39\n",
      "Completed haber-gpt\\trainer_state.bin\n",
      "iter 1060: loss 2.6675, run_iter_time 1144.03ms, fb_toks/sec 14092.93, run_fb_toks/sec 14490.24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m HFBackedTrainer\u001b[38;5;241m.\u001b[39mfrom_config(trainer_cfg_file)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\hf_backed_trainer.py:66\u001b[0m, in \u001b[0;36mHFBackedTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     63\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_state)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model state dict!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# wait until jobs are complete\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\trainer.py:227\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    224\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    226\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 227\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[0;32m    229\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = HFBackedTrainer.from_config(trainer_cfg_file)\n",
    "trainer.train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
