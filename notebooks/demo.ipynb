{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340f221cda68499f8c2fe44c71f2559e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuseyinabanozis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 13.77M\n",
      "Resume training. Using repo {config.repo_id}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\wandb\\run-20240718_193713-rhglh6el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huseyinabanozis/GPT%20Training/runs/rhglh6el' target=\"_blank\">train-nano-1-bf16fix-lr-3</a></strong> to <a href='https://wandb.ai/huseyinabanozis/GPT%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/huseyinabanozis/GPT%20Training' target=\"_blank\">https://wandb.ai/huseyinabanozis/GPT%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/huseyinabanozis/GPT%20Training/runs/rhglh6el' target=\"_blank\">https://wandb.ai/huseyinabanozis/GPT%20Training/runs/rhglh6el</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 13,959,168 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\model\\gpt2.py:67: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback iter: 0, best val loss: 9.063164710998535\n",
      "Eval iter 0: train loss 9.0573, val loss 9.0632, ETA 0:00:00\n",
      "iter 0: loss 9.0643, run_iter_time 733.61ms, fb_toks/sec 22333.53, run_fb_toks/sec 22333.53\n",
      "iter 2: loss 8.1259, run_iter_time 732.85ms, fb_toks/sec 22566.12, run_fb_toks/sec 22356.79\n",
      "iter 4: loss 7.6198, run_iter_time 731.62ms, fb_toks/sec 22738.36, run_fb_toks/sec 22394.95\n",
      "Callback iter: 5, best val loss: 7.276614189147949\n",
      "saving checkpoint to news-out\n",
      "Eval iter 5: train loss 7.3307, val loss 7.2766, ETA 0:00:06.496478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb630fbdcc4e0f9bc670caf2ac62e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/55.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd44c23c6bc4df881244c05ccb32cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/112M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6: loss 7.1955, run_iter_time 731.11ms, fb_toks/sec 22549.64, run_fb_toks/sec 22410.42\n",
      "iter 8: loss 6.9998, run_iter_time 731.41ms, fb_toks/sec 22320.11, run_fb_toks/sec 22401.39\n",
      "Callback iter: 10, best val loss: 6.714324951171875\n",
      "saving checkpoint to news-out\n",
      "Completed news-out\\pytorch_model.bin\n",
      "Completed news-out\\trainer_state.bin\n",
      "Eval iter 10: train loss 6.7302, val loss 6.7143, ETA 0:00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4686ef0cf67d4011ae304281b97e03fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/55.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833eaf96e3514403b96cfe05c6edfd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/112M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10: loss 6.8800, run_iter_time 748.97ms, fb_toks/sec 18063.79, run_fb_toks/sec 21967.63\n",
      "Completed news-out\\pytorch_model.bin\n",
      "Completed news-out\\trainer_state.bin\n"
     ]
    }
   ],
   "source": [
    "from hf_backed_trainer import HFBackedTrainer\n",
    "from model.gpt2 import GPT\n",
    "\n",
    "model = GPT.from_config(\"config/news_model.yml\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "trainer = HFBackedTrainer.from_config(\"config/news_trainer.yml\")\n",
    "trainer.train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
