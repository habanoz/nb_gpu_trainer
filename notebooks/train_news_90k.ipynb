{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/habanoz/Desktop/gpt/nb_gpu_trainer\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: transformers in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from nb_gpu_trainer==1.0) (4.42.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from nb_gpu_trainer==1.0) (2.20.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from nb_gpu_trainer==1.0) (0.23.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from nb_gpu_trainer==1.0) (0.2.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from nb_gpu_trainer==1.0) (0.17.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->nb_gpu_trainer==1.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (3.9.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets->nb_gpu_trainer==1.0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub->nb_gpu_trainer==1.0) (4.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from transformers->nb_gpu_trainer==1.0) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from transformers->nb_gpu_trainer==1.0) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from transformers->nb_gpu_trainer==1.0) (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (5.27.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (2.7.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from wandb->nb_gpu_trainer==1.0) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb->nb_gpu_trainer==1.0) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb->nb_gpu_trainer==1.0) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nb_gpu_trainer==1.0) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets->nb_gpu_trainer==1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets->nb_gpu_trainer==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets->nb_gpu_trainer==1.0) (2024.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\habanoz\\miniconda3\\envs\\myenv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nb_gpu_trainer==1.0) (5.0.1)\n",
      "Building wheels for collected packages: nb_gpu_trainer\n",
      "  Building editable for nb_gpu_trainer (pyproject.toml): started\n",
      "  Building editable for nb_gpu_trainer (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nb_gpu_trainer: filename=nb_gpu_trainer-1.0-0.editable-py3-none-any.whl size=2171 sha256=002f6d83681bb9a96a7b826365d7ce315abb4a8469171a54792201888cde08b9\n",
      "  Stored in directory: C:\\Users\\habanoz\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-xeef896b\\wheels\\83\\aa\\fd\\a4d94c85b2eecd7db2a75e22b86f7cf018745b50bcf5b0c613\n",
      "Successfully built nb_gpu_trainer\n",
      "Installing collected packages: nb_gpu_trainer\n",
      "  Attempting uninstall: nb_gpu_trainer\n",
      "    Found existing installation: nb_gpu_trainer 1.0\n",
      "    Uninstalling nb_gpu_trainer-1.0:\n",
      "      Successfully uninstalled nb_gpu_trainer-1.0\n",
      "Successfully installed nb_gpu_trainer-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read confing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbtr.model.gpt2 import GPTConfig\n",
    "from nbtr.trainer import TrainerConfig\n",
    "from nbtr.tokenizer.tokenizer import Tokenizer\n",
    "from nbtr.tokenizer.train import Trainer as tok_trainer\n",
    "from nbtr.hf_backed_trainer import HFBackedTrainer\n",
    "from nbtr.model.hf_model import HfModel\n",
    "from nbtr.model.hf_model_config import HfModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6ca78753604c9ba824e98dcf45c14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_file = \"config/news_model.yml\"\n",
    "train_config_file = \"config/news_trainer.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config =  TrainerConfig.from_yaml(train_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('haber-90k-gpt',\n",
       " 'habanoz/haber-90k-gpt-v1.1_test',\n",
       " 'habanoz/haber-90k-gpt-text',\n",
       " 'haber-90k-data-test')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config.out_dir, trainer_config.repo_id, trainer_config.ds_repo_id, trainer_config.data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Trainer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tok_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtok_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Trainer' object is not callable"
     ]
    }
   ],
   "source": [
    "_tok_trainer = tok_trainer(trainer_config.ds_repo_id, trainer_config.repo_id, trainer_config.out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created repo https://huggingface.co/habanoz/haber-90k-gpt-v1.1_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e740befc1b88490e9fccf53d28ab7996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded to the repo at location habanoz/haber-90k-gpt-v1.1_test\n"
     ]
    }
   ],
   "source": [
    "_tok_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641dcc18862c4a949c0fa6fde8a0a0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\habanoz\\.cache\\huggingface\\hub\\models--habanoz--haber-90k-gpt-v1.1_test. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(trainer_config.repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'train' tokens: 62748074\n",
      "Number of 'validation' tokens: 7015414\n"
     ]
    }
   ],
   "source": [
    "tokenizer.encode_ds_from_hub(trainer_config.ds_repo_id, trainer_config.data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335dbc0ed68d4dc6a970c3e2e771dc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\habanoz\\.cache\\huggingface\\hub\\models--habanoz--haber-90k-gpt-v1.1_test. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 13.77M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d662fdc0a29d4511aa552b435285cd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/56.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HfModel(\n",
       "  (_model): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(8192, 384)\n",
       "      (wpe): Embedding(1024, 384)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x Block(\n",
       "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=384, out_features=8192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_config = HfModelConfig(GPTConfig.from_yaml(model_config_file))\n",
    "hf_model = HfModel.from_pretrained(trainer_config.repo_id)\n",
    "hf_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_config.save_pretrained(\"haber-90k-gpt-v1.1_test\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4955044a159c4fec9547099ebb74bda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer state is not none\n",
      "Resume training. Using repo habanoz/haber-90k-gpt-v1.1_test\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainerConfig.from_yaml(train_config_file)\n",
    "trainer = HFBackedTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 14,155,776 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\model\\gpt2.py:76: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback iter: 500, best val loss: 4.46705961227417\n",
      "saving checkpoint to haber-90k-gpt\n",
      "Eval iter 500: train loss 4.4530, val loss 4.4671, ETA 0:00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea311335c42544c1a0a35dccbb145af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.bin:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 500: loss 4.6070, run_iter_time 3512.03ms, fb_toks/sec 9330.22, run_fb_toks/sec 9330.22\n",
      "iter 510: loss 4.4455, run_iter_time 3369.03ms, fb_toks/sec 15738.70, run_fb_toks/sec 9971.07\n",
      "iter 520: loss 4.5598, run_iter_time 3231.02ms, fb_toks/sec 16474.89, run_fb_toks/sec 10621.45\n",
      "iter 530: loss 4.4908, run_iter_time 3109.82ms, fb_toks/sec 16229.66, run_fb_toks/sec 11182.27\n",
      "iter 540: loss 4.4261, run_iter_time 3004.87ms, fb_toks/sec 15904.66, run_fb_toks/sec 11654.51\n",
      "iter 550: loss 4.4260, run_iter_time 2908.08ms, fb_toks/sec 16086.08, run_fb_toks/sec 12097.67\n",
      "iter 560: loss 4.4441, run_iter_time 2838.18ms, fb_toks/sec 14833.84, run_fb_toks/sec 12371.29\n",
      "iter 570: loss 4.3166, run_iter_time 2764.16ms, fb_toks/sec 15618.42, run_fb_toks/sec 12696.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\hf_backed_trainer.py:60\u001b[0m, in \u001b[0;36mHFBackedTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     57\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_state)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model state dict!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# wait until jobs are complete\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\trainer.py:242\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    245\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(hf_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 14,155,776 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\model\\gpt2.py:76: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval iter 500: train loss 9.0735, val loss 9.0734, ETA 0:00:00\n",
      "iter 500: loss 9.0764, run_iter_time 3772.57ms, fb_toks/sec 8685.86, run_fb_toks/sec 8685.86\n",
      "iter 510: loss 7.6443, run_iter_time 3598.12ms, fb_toks/sec 16157.37, run_fb_toks/sec 9433.01\n",
      "iter 520: loss 7.7173, run_iter_time 3459.50ms, fb_toks/sec 14813.66, run_fb_toks/sec 9971.08\n",
      "iter 530: loss 7.6600, run_iter_time 3354.84ms, fb_toks/sec 13580.43, run_fb_toks/sec 10332.01\n",
      "iter 540: loss 7.5307, run_iter_time 3253.46ms, fb_toks/sec 13997.37, run_fb_toks/sec 10698.55\n"
     ]
    }
   ],
   "source": [
    "trainer.train(hf_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 14,155,776 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\model\\gpt2.py:76: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval iter 500: train loss 9.0808, val loss 9.0817, ETA 0:00:00\n",
      "iter 500: loss 9.0793, run_iter_time 3864.02ms, fb_toks/sec 8480.29, run_fb_toks/sec 8480.29\n",
      "iter 510: loss 7.6431, run_iter_time 3673.96ms, fb_toks/sec 16688.89, run_fb_toks/sec 9301.15\n",
      "iter 520: loss 7.7132, run_iter_time 3506.57ms, fb_toks/sec 16383.57, run_fb_toks/sec 10009.39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\hf_backed_trainer.py:62\u001b[0m, in \u001b[0;36mHFBackedTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_state)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model state dict!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# wait until jobs are complete\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\trainer.py:242\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    245\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(hf_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 14,155,776 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\model\\gpt2.py:76: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval iter 500: train loss 9.0589, val loss 9.0590, ETA 0:00:00\n",
      "iter 500: loss 9.0638, run_iter_time 3804.09ms, fb_toks/sec 8613.88, run_fb_toks/sec 8613.88\n",
      "iter 510: loss 7.6491, run_iter_time 3651.38ms, fb_toks/sec 14390.79, run_fb_toks/sec 9191.57\n",
      "iter 520: loss 7.7009, run_iter_time 3503.95ms, fb_toks/sec 15051.81, run_fb_toks/sec 9777.60\n",
      "iter 530: loss 7.6508, run_iter_time 3376.33ms, fb_toks/sec 14708.65, run_fb_toks/sec 10270.70\n",
      "iter 540: loss 7.5347, run_iter_time 3249.80ms, fb_toks/sec 15522.68, run_fb_toks/sec 10795.90\n",
      "iter 550: loss 7.4411, run_iter_time 3158.02ms, fb_toks/sec 14051.15, run_fb_toks/sec 11121.42\n",
      "iter 560: loss 7.3552, run_iter_time 3093.02ms, fb_toks/sec 13065.27, run_fb_toks/sec 11315.81\n",
      "iter 570: loss 7.2336, run_iter_time 3029.52ms, fb_toks/sec 13331.29, run_fb_toks/sec 11517.36\n",
      "iter 580: loss 7.1971, run_iter_time 2978.37ms, fb_toks/sec 13013.41, run_fb_toks/sec 11666.96\n",
      "iter 590: loss 7.0704, run_iter_time 2895.34ms, fb_toks/sec 15254.76, run_fb_toks/sec 12025.74\n",
      "iter 600: loss 6.9216, run_iter_time 2839.71ms, fb_toks/sec 14009.08, run_fb_toks/sec 12224.08\n",
      "iter 610: loss 6.8218, run_iter_time 2781.29ms, fb_toks/sec 14528.17, run_fb_toks/sec 12454.48\n",
      "iter 620: loss 6.7059, run_iter_time 2742.65ms, fb_toks/sec 13681.95, run_fb_toks/sec 12577.23\n",
      "iter 630: loss 6.6245, run_iter_time 2678.20ms, fb_toks/sec 15617.84, run_fb_toks/sec 12881.29\n",
      "iter 640: loss 6.4718, run_iter_time 2649.04ms, fb_toks/sec 13730.23, run_fb_toks/sec 12966.19\n",
      "iter 650: loss 6.2813, run_iter_time 2612.24ms, fb_toks/sec 14365.32, run_fb_toks/sec 13106.10\n",
      "iter 660: loss 6.2980, run_iter_time 2567.02ms, fb_toks/sec 15170.28, run_fb_toks/sec 13312.52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\hf_backed_trainer.py:62\u001b[0m, in \u001b[0;36mHFBackedTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_state)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model state dict!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# wait until jobs are complete\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\trainer.py:242\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    245\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\habanoz\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train(hf_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 14,155,776 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habanoz\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\model\\gpt2.py:76: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback iter: 0, best val loss: 9.085393905639648\n",
      "Eval iter 0: train loss 9.0863, val loss 9.0854, ETA 0:00:00\n",
      "iter 0: loss 9.0898, run_iter_time 1644.97ms, fb_toks/sec 4980.02, run_fb_toks/sec 4980.02\n",
      "iter 10: loss 8.5029, run_iter_time 1660.27ms, fb_toks/sec 4556.24, run_fb_toks/sec 4937.65\n",
      "iter 20: loss 8.1967, run_iter_time 1663.55ms, fb_toks/sec 4838.71, run_fb_toks/sec 4927.75\n",
      "iter 30: loss 7.7021, run_iter_time 1677.89ms, fb_toks/sec 4533.46, run_fb_toks/sec 4888.32\n",
      "iter 40: loss 7.3702, run_iter_time 1691.20ms, fb_toks/sec 4523.48, run_fb_toks/sec 4851.84\n",
      "iter 50: loss 6.8880, run_iter_time 1714.89ms, fb_toks/sec 4248.86, run_fb_toks/sec 4791.54\n",
      "iter 60: loss 6.9289, run_iter_time 1734.61ms, fb_toks/sec 4284.16, run_fb_toks/sec 4740.80\n",
      "iter 70: loss 6.7574, run_iter_time 1777.75ms, fb_toks/sec 3782.06, run_fb_toks/sec 4644.93\n",
      "iter 80: loss 6.4745, run_iter_time 1788.78ms, fb_toks/sec 4339.05, run_fb_toks/sec 4614.34\n",
      "iter 90: loss 6.3318, run_iter_time 1797.94ms, fb_toks/sec 4356.41, run_fb_toks/sec 4588.55\n",
      "iter 100: loss 5.8716, run_iter_time 1808.15ms, fb_toks/sec 4311.46, run_fb_toks/sec 4560.84\n",
      "iter 110: loss 5.9757, run_iter_time 1820.54ms, fb_toks/sec 4240.14, run_fb_toks/sec 4528.77\n",
      "iter 120: loss 5.8019, run_iter_time 1833.19ms, fb_toks/sec 4207.40, run_fb_toks/sec 4496.63\n",
      "iter 130: loss 5.7997, run_iter_time 1846.57ms, fb_toks/sec 4164.71, run_fb_toks/sec 4463.44\n",
      "iter 140: loss 5.5090, run_iter_time 1851.12ms, fb_toks/sec 4329.75, run_fb_toks/sec 4450.07\n",
      "iter 150: loss 5.4779, run_iter_time 1857.01ms, fb_toks/sec 4288.89, run_fb_toks/sec 4433.95\n",
      "iter 160: loss 5.6370, run_iter_time 1872.21ms, fb_toks/sec 4077.63, run_fb_toks/sec 4398.32\n",
      "iter 170: loss 5.2210, run_iter_time 1886.49ms, fb_toks/sec 4065.45, run_fb_toks/sec 4365.03\n",
      "iter 180: loss 5.2531, run_iter_time 1895.92ms, fb_toks/sec 4135.80, run_fb_toks/sec 4342.11\n",
      "iter 190: loss 5.4339, run_iter_time 1903.63ms, fb_toks/sec 4152.05, run_fb_toks/sec 4323.10\n",
      "iter 200: loss 4.9535, run_iter_time 1906.77ms, fb_toks/sec 4233.57, run_fb_toks/sec 4314.15\n",
      "iter 210: loss 5.3797, run_iter_time 1901.39ms, fb_toks/sec 4420.91, run_fb_toks/sec 4324.83\n",
      "iter 220: loss 5.2436, run_iter_time 1898.31ms, fb_toks/sec 4379.38, run_fb_toks/sec 4330.28\n",
      "iter 230: loss 5.0546, run_iter_time 1907.31ms, fb_toks/sec 4120.14, run_fb_toks/sec 4309.27\n",
      "iter 240: loss 5.1656, run_iter_time 1920.58ms, fb_toks/sec 4015.66, run_fb_toks/sec 4279.91\n",
      "Callback iter: 250, best val loss: 5.0549798011779785\n",
      "saving checkpoint to haber-90k-gpt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HfModel.save() missing 1 required positional argument: 'trainer_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\hf_backed_trainer.py:60\u001b[0m, in \u001b[0;36mHFBackedTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     57\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_state)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model state dict!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# wait until jobs are complete\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\trainer.py:230\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    227\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_lr(it, optimizer)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_fwd_bwd_tokens_per_sec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_iter_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[0;32m    233\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\trainer.py:289\u001b[0m, in \u001b[0;36mTrainer.do_eval\u001b[1;34m(self, model, optimizer, running_fwd_bwd_tokens_per_sec, time_per_iter, it, lr)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  new_val_loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_val_loss:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m TrainingState(iter_num\u001b[38;5;241m=\u001b[39mit, best_val_loss\u001b[38;5;241m=\u001b[39mnew_val_loss, optim_state\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_new_best_val_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    292\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\trainer.py:84\u001b[0m, in \u001b[0;36mTrainer.trigger_callbacks\u001b[1;34m(self, onevent, model)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrigger_callbacks\u001b[39m(\u001b[38;5;28mself\u001b[39m, onevent: \u001b[38;5;28mstr\u001b[39m, model):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(onevent, []):\n\u001b[1;32m---> 84\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\hf_backed_trainer.py:37\u001b[0m, in \u001b[0;36mHFBackedTrainer.__init__.<locals>.<lambda>\u001b[1;34m(trainer, model)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew training. Created repo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config\u001b[38;5;241m=\u001b[39mconfig, state\u001b[38;5;241m=\u001b[39mtrainer_state)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_new_best_val_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m trainer, model : \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_on_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor \u001b[38;5;241m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers)\n",
      "File \u001b[1;32m~\\Desktop\\gpt\\nb_gpu_trainer\\src\\nbtr\\hf_backed_trainer.py:85\u001b[0m, in \u001b[0;36mHFBackedTrainer.do_on_eval\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     83\u001b[0m hf_state\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# save model state\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43mHfModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# upload saved files at the background\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor \u001b[38;5;241m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers)\n",
      "\u001b[1;31mTypeError\u001b[0m: HfModel.save() missing 1 required positional argument: 'trainer_config'"
     ]
    }
   ],
   "source": [
    "trainer.train(hf_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
