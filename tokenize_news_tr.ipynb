{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/melek/workspace/vscode/nb_gpu_trainer-1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from nb_gpu_trainer==1.0) (4.41.2)\n",
      "Requirement already satisfied: datasets in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from nb_gpu_trainer==1.0) (2.20.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from nb_gpu_trainer==1.0) (0.23.2)\n",
      "Requirement already satisfied: sentencepiece in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from nb_gpu_trainer==1.0) (0.2.0)\n",
      "Requirement already satisfied: wandb in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from nb_gpu_trainer==1.0) (0.17.5)\n",
      "Requirement already satisfied: filelock in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->nb_gpu_trainer==1.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (3.9.5)\n",
      "Requirement already satisfied: packaging in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets->nb_gpu_trainer==1.0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub->nb_gpu_trainer==1.0) (4.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from transformers->nb_gpu_trainer==1.0) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from transformers->nb_gpu_trainer==1.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from transformers->nb_gpu_trainer==1.0) (0.4.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (2.10.0)\n",
      "Requirement already satisfied: setproctitle in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from wandb->nb_gpu_trainer==1.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb->nb_gpu_trainer==1.0) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets->nb_gpu_trainer==1.0) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nb_gpu_trainer==1.0) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets->nb_gpu_trainer==1.0) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from pandas->datasets->nb_gpu_trainer==1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from pandas->datasets->nb_gpu_trainer==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from pandas->datasets->nb_gpu_trainer==1.0) (2024.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nb_gpu_trainer==1.0) (5.0.1)\n",
      "Building wheels for collected packages: nb_gpu_trainer\n",
      "  Building editable for nb_gpu_trainer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nb_gpu_trainer: filename=nb_gpu_trainer-1.0-0.editable-py3-none-any.whl size=2142 sha256=05303fdeeb0a48cf6dcc1eeb8b4e4e2006a8daede0d6c33bd6ff78e45fcb2bc8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p2a5jgkj/wheels/e5/15/6d/efad4a44556fd230100c1915c1eacdff58d68ac998cf87023b\n",
      "Successfully built nb_gpu_trainer\n",
      "Installing collected packages: nb_gpu_trainer\n",
      "  Attempting uninstall: nb_gpu_trainer\n",
      "    Found existing installation: nb_gpu_trainer 1.0\n",
      "    Uninstalling nb_gpu_trainer-1.0:\n",
      "      Successfully uninstalled nb_gpu_trainer-1.0\n",
      "Successfully installed nb_gpu_trainer-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation split is found. Creating a validation split.\n",
      "Validation split created.\n",
      "Generating tokens!\n",
      "Split train, token count: 4481201\n",
      "memmap is ready!\n",
      "Saving split 'train', docs: 7136, tokens: 4481201 in 1024 batches.\n",
      "writing eco-news-tr/train.bin: 100%|██████| 1024/1024 [00:00<00:00, 1175.79it/s]\n",
      "Saved 'train' tokens.\n",
      "Split val, token count: 270469\n",
      "memmap is ready!\n",
      "Saving split 'val', docs: 416, tokens: 270469 in 1 batches.\n",
      "writing eco-news-tr/val.bin: 100%|███████████████| 1/1 [00:00<00:00, 303.19it/s]\n",
      "Saved 'val' tokens.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python -m nbtr.cli_generate_data --repo_id habanoz/news-tr-1.8M-tokenizer --ds_repo_id habanoz/eco-news-tr --data_dir eco-news-tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████| 635/635 [00:00<00:00, 7.44MB/s]\n",
      "Downloading data: 100%|██████████████████████| 262M/262M [00:24<00:00, 10.7MB/s]\n",
      "Downloading data: 100%|██████████████████████| 277M/277M [00:25<00:00, 11.0MB/s]\n",
      "Downloading data: 100%|██████████████████████| 295M/295M [00:26<00:00, 11.0MB/s]\n",
      "Downloading data: 100%|██████████████████████| 252M/252M [00:23<00:00, 10.9MB/s]\n",
      "Downloading data: 100%|██████████████████████| 295M/295M [00:26<00:00, 11.3MB/s]\n",
      "Downloading data: 100%|██████████████████████| 262M/262M [00:23<00:00, 11.1MB/s]\n",
      "Downloading data: 100%|██████████████████████| 273M/273M [00:24<00:00, 11.2MB/s]\n",
      "Downloading data: 100%|██████████████████████| 291M/291M [00:26<00:00, 10.8MB/s]\n",
      "Downloading data: 100%|██████████████████████| 252M/252M [00:22<00:00, 11.1MB/s]\n",
      "Downloading data: 100%|██████████████████████| 285M/285M [00:26<00:00, 10.7MB/s]\n",
      "Generating train split: 100%|█| 1845941/1845941 [00:14<00:00, 123506.66 examples\n",
      "No validation split is found. Creating a validation split.\n",
      "Validation split created.\n",
      "Generating tokens!\n",
      "tokenizing the splits: 100%|█| 1744414/1744414 [11:53<00:00, 2444.42 examples/s]\n",
      "tokenizing the splits: 100%|███| 101527/101527 [00:31<00:00, 3202.34 examples/s]\n",
      "Split train, token count: 823598299\n",
      "memmap is ready!\n",
      "Saving split 'train', docs: 1744414, tokens: 823598299 in 1048576 batches.\n",
      "writing news-tr-1.8M-tokenizer/train.bin: 100%|█| 1048576/1048576 [22:29<00:00, \n",
      "Saved 'train' tokens.\n",
      "Split val, token count: 47698095\n",
      "memmap is ready!\n",
      "Saving split 'val', docs: 101527, tokens: 47698095 in 1024 batches.\n",
      "writing news-tr-1.8M-tokenizer/val.bin: 100%|█| 1024/1024 [00:02<00:00, 474.11it\n",
      "Saved 'val' tokens.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python -m nbtr.cli_generate_data --repo_id habanoz/news-tr-1.8M-tokenizer --ds_repo_id habanoz/news-tr-1.8M --data_dir news-tr-1.8M-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation split is found. Creating a validation split.\n",
      "Validation split created.\n",
      "Generating tokens!\n",
      "Split train, token count: 903252685\n",
      "memmap is ready!\n",
      "Saving split 'train', docs: 1744414, tokens: 903252685 in 1048576 batches.\n",
      "writing news-tr-1.8M-tokenizer-16k/train.bin: 100%|█| 1048576/1048576 [21:58<00:\n",
      "Saved 'train' tokens.\n",
      "Split val, token count: 52314787\n",
      "memmap is ready!\n",
      "Saving split 'val', docs: 101527, tokens: 52314787 in 1024 batches.\n",
      "writing news-tr-1.8M-tokenizer-16k/val.bin: 100%|█| 1024/1024 [00:02<00:00, 488.\n",
      "Saved 'val' tokens.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python -m nbtr.cli_generate_data --repo_id habanoz/news-tr-1.8M-tokenizer-16k --ds_repo_id habanoz/news-tr-1.8M --data_dir news-tr-1.8M-tokenizer-16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation split is found. Creating a validation split.\n",
      "Validation split created.\n",
      "Generating tokens!\n",
      "tokenizing the splits: 100%|█| 1744414/1744414 [15:29<00:00, 1876.03 examples/s]\n",
      "tokenizing the splits: 100%|███| 101527/101527 [00:37<00:00, 2680.80 examples/s]\n",
      "Split train, token count: 1013631461\n",
      "memmap is ready!\n",
      "Saving split 'train', docs: 1744414, tokens: 1013631461 in 1048576 batches.\n",
      "writing news-tr-1.8M-tokenizer-8k/train.bin: 100%|█| 1048576/1048576 [27:02<00:0\n",
      "Saved 'train' tokens.\n",
      "Split val, token count: 58712448\n",
      "memmap is ready!\n",
      "Saving split 'val', docs: 101527, tokens: 58712448 in 1024 batches.\n",
      "writing news-tr-1.8M-tokenizer-8k/val.bin: 100%|█| 1024/1024 [00:57<00:00, 17.82\n",
      "Saved 'val' tokens.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python -m nbtr.cli_generate_data --repo_id habanoz/news-tr-1.8M-tokenizer-8k --ds_repo_id habanoz/news-tr-1.8M --data_dir news-tr-1.8M-tokenizer-8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('habanoz/news-tr-1.8M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'url', 'tokens', 'length'],\n",
       "        num_rows: 1845941\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '10. Büyükelçiler Konferansı\\nPutin ve Muhammed bin Selman petrol piyasalarını ve Ukrayna\\'daki durumu görüştü\\nPutin ve Muhammed bin Selman petrol piyasalarını ve Ukrayna\\'daki durumu görüştü\\'Türk iş adamlarının Gine\\'de yapacağı çok şey var\\'\\nTürkiye\\'nin Konakri Büyükelçisi Sağman, \"Bizim Afrika\\'ya bakış açımız; eşit seviyede birlikte yol katetmek. Gineliler haliyle bu samimiyetin farkındalar ve bunun meyvelerini de görüyoruz.\" dedi.\\n\\'Maarif Vakfı\\'na devredilen okullar örnek oluyor\\'\\nTürkiye\\'nin Abidjan Büyükelçisi Esra Demir, \"Kapatılan ya da Maarif Vakfı\\'na devredilen her okul (FETÖ\\'ye ait), Afrika bölgesindeki diğer ülkelerde benzer adımların atılması için örnek oluyor.\" dedi.\\n\\'Tanzanya\\'da Türk müteahhitler listenin ilk sırasında\\'\\nTürkiye\\'nin Darüsselam Büyükelçisi Ali Davutoğlu, Tanzanya ile Türkiye ilişkilerinin geliştirilmesi açısından büyük imkanların mevcut olduğunu kaydederek, Türkiye\\'nin müteahhitlik alanında en yüksek taahhüt aldığı ülkenin Tanzanya olduğunu söyledi.\\n\\'Biz Somali\\'de hiçbir aktörle rekabet içinde olmadık\\'\\nTürkiye\\'nin Mogadişu Büyükelçisi Olgan Bekar, \"Biz, Somali\\'de hiçbir aktörle rekabet içinde olmadık. Böyle bir çabamız hiçbir zaman olmadı.\" dedi.',\n",
       " 'url': 'https://www.aa.com.tr/tr/10-buyukelciler-konferansi',\n",
       " 'tokens': 588,\n",
       " 'length': 1187}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train'][0]['text'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.',\n",
       " 'Büyükelçiler',\n",
       " 'Konferansı',\n",
       " 'Putin',\n",
       " 've',\n",
       " 'Muhammed',\n",
       " 'bin',\n",
       " 'Selman',\n",
       " 'petrol',\n",
       " 'piyasalarını',\n",
       " 've',\n",
       " \"Ukrayna'daki\",\n",
       " 'durumu',\n",
       " 'görüştü',\n",
       " 'Putin',\n",
       " 've',\n",
       " 'Muhammed',\n",
       " 'bin',\n",
       " 'Selman',\n",
       " 'petrol',\n",
       " 'piyasalarını',\n",
       " 've',\n",
       " \"Ukrayna'daki\",\n",
       " 'durumu',\n",
       " \"görüştü'Türk\",\n",
       " 'iş',\n",
       " 'adamlarının',\n",
       " \"Gine'de\",\n",
       " 'yapacağı',\n",
       " 'çok',\n",
       " 'şey',\n",
       " \"var'\",\n",
       " \"Türkiye'nin\",\n",
       " 'Konakri',\n",
       " 'Büyükelçisi',\n",
       " 'Sağman,',\n",
       " '\"Bizim',\n",
       " \"Afrika'ya\",\n",
       " 'bakış',\n",
       " 'açımız;',\n",
       " 'eşit',\n",
       " 'seviyede',\n",
       " 'birlikte',\n",
       " 'yol',\n",
       " 'katetmek.',\n",
       " 'Gineliler',\n",
       " 'haliyle',\n",
       " 'bu',\n",
       " 'samimiyetin',\n",
       " 'farkındalar',\n",
       " 've',\n",
       " 'bunun',\n",
       " 'meyvelerini',\n",
       " 'de',\n",
       " 'görüyoruz.\"',\n",
       " 'dedi.',\n",
       " \"'Maarif\",\n",
       " \"Vakfı'na\",\n",
       " 'devredilen',\n",
       " 'okullar',\n",
       " 'örnek',\n",
       " \"oluyor'\",\n",
       " \"Türkiye'nin\",\n",
       " 'Abidjan',\n",
       " 'Büyükelçisi',\n",
       " 'Esra',\n",
       " 'Demir,',\n",
       " '\"Kapatılan',\n",
       " 'ya',\n",
       " 'da',\n",
       " 'Maarif',\n",
       " \"Vakfı'na\",\n",
       " 'devredilen',\n",
       " 'her',\n",
       " 'okul',\n",
       " \"(FETÖ'ye\",\n",
       " 'ait),',\n",
       " 'Afrika',\n",
       " 'bölgesindeki',\n",
       " 'diğer',\n",
       " 'ülkelerde',\n",
       " 'benzer',\n",
       " 'adımların',\n",
       " 'atılması',\n",
       " 'için',\n",
       " 'örnek',\n",
       " 'oluyor.\"',\n",
       " 'dedi.',\n",
       " \"'Tanzanya'da\",\n",
       " 'Türk',\n",
       " 'müteahhitler',\n",
       " 'listenin',\n",
       " 'ilk',\n",
       " \"sırasında'\",\n",
       " \"Türkiye'nin\",\n",
       " 'Darüsselam',\n",
       " 'Büyükelçisi',\n",
       " 'Ali',\n",
       " 'Davutoğlu,',\n",
       " 'Tanzanya',\n",
       " 'ile',\n",
       " 'Türkiye',\n",
       " 'ilişkilerinin',\n",
       " 'geliştirilmesi',\n",
       " 'açısından',\n",
       " 'büyük',\n",
       " 'imkanların',\n",
       " 'mevcut',\n",
       " 'olduğunu',\n",
       " 'kaydederek,',\n",
       " \"Türkiye'nin\",\n",
       " 'müteahhitlik',\n",
       " 'alanında',\n",
       " 'en',\n",
       " 'yüksek',\n",
       " 'taahhüt',\n",
       " 'aldığı',\n",
       " 'ülkenin',\n",
       " 'Tanzanya',\n",
       " 'olduğunu',\n",
       " 'söyledi.',\n",
       " \"'Biz\",\n",
       " \"Somali'de\",\n",
       " 'hiçbir',\n",
       " 'aktörle',\n",
       " 'rekabet',\n",
       " 'içinde',\n",
       " \"olmadık'\",\n",
       " \"Türkiye'nin\",\n",
       " 'Mogadişu',\n",
       " 'Büyükelçisi',\n",
       " 'Olgan',\n",
       " 'Bekar,',\n",
       " '\"Biz,',\n",
       " \"Somali'de\",\n",
       " 'hiçbir',\n",
       " 'aktörle',\n",
       " 'rekabet',\n",
       " 'içinde',\n",
       " 'olmadık.',\n",
       " 'Böyle',\n",
       " 'bir',\n",
       " 'çabamız',\n",
       " 'hiçbir',\n",
       " 'zaman',\n",
       " 'olmadı.\"',\n",
       " 'dedi.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['text'].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1684016631"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( len(encoding.encode(row['text'])) for row in ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News-tr-32k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/melek/workspace/vscode/nb_gpu_trainer-1/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbtr.tokenizer.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"habanoz/news-tr-1.8M-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nbtr.tokenizer.tokenizer.Tokenizer at 0x701fd8183d90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(ds['train'][0]['text'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(ds['train'][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871296394"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( len(tokenizer.encode(row['text'])['input_ids']) for row in ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newstr-16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer16k = Tokenizer.from_pretrained(\"habanoz/news-tr-1.8M-tokenizer-16k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nbtr.tokenizer.tokenizer.Tokenizer at 0x701fc00cb7d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955567472"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( len(tokenizer16k.encode(row['text'])['input_ids']) for row in ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newstr-8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer8k = Tokenizer.from_pretrained(\"habanoz/news-tr-1.8M-tokenizer-8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1072343909"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( len(tokenizer8k.encode(row['text'])['input_ids']) for row in ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1072343909"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( len(tokenizer8k.encode_raw(row['text'])) for row in ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068652027"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( len(tokenizer8k.encode_raw(row['text'], False, False)) for row in ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
