
seq_length: 1024
gradient_accumulation_steps: 2
batch_size: 16
max_iters : 100
lr_decay_iters : 100
warmup_iters: 1
dtype: float16
compile: False
    
# learning rate
learning_rate:  0.001
decay_lr: True
min_lr: 0.0001

# optimizer
weight_decay: 0.1
beta1:  0.9
beta2: 0.99

# logging
log_interval: 10
eval_interval: 100 # must be multiple of log interval
eval_iters: 1
promised_flops : 65.0e+12 # Tesla T4 on fp16
## wandb logging
wandb_log: False
wandb_project: NB-Haber-GPT-2-Training
wandb_run_name: haber-gpt-v2-initial
wandb_run_id: "1728306339"

# SxBxW -> 1024×32×2 = 65536
# 19200×65536 = 1,258,291,200