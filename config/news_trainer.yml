
seq_length: 512
gradient_accumulation_steps: 1
batch_size: 128
max_iters : 4000
lr_decay_iters : 4000
warmup_iters: 100
dtype: float16
compile: False
    
# learning rate
learning_rate:  0.003
decay_lr: True
min_lr: 0.0001

# optimizer
weight_decay: 0.1
beta1:  0.9
beta2: 0.99

# logging
log_interval: 100
eval_interval: 500 # must be multiple of log interval
eval_iters: 100
promised_flops : 65.0e+12 # Tesla T4 on fp16
## wandb logging
wandb_log: True
wandb_project: Haber-GPT-2-Small
wandb_run_name: haber-gpt-2-small-v1.0
wandb_run_id: "1728306340"

# eval budget
# 512 (seq len) * 128 (eval batch size) * 100 (eval iterations) * 9 (8+1 evals) = 58,982,400 ~ 60M tokens
# flops = 60M * N

# training budget
# 512 (seq len) * 128 (batch size) * 1 (grad accumulations) * 4000 (training iterations) = 262,144,000 ~ 262M tokens
# flops = 262M * 3N

# ratio
# 60M*N / 262M * 3N = 0.076335878 ~ 8%

# training data
# news-tr-1.8M-tokenizer-8k
# 1UsURw9L0t-BWlvM2IigNIeGObQn89FMI