seq_length: 1024
batch_size: 32
gradient_accumulation_steps: 1
ds_repo_id : habanoz/haber-90k-gpt-text
warmup_iters: 100
learning_rate:  0.001
lr_decay_iters: 6000
max_iters: 6000
min_lr: 0.0001
weight_decay: 0.1
beta1:  0.9
beta2: 0.99
compile: False
decay_lr: True
grad_norm_clip:  1.0
seed: 145
# output
out_dir: haber-90k-gpt
# logging 
log_interval: 10
eval_interval: 250
eval_iters: 200
wandb_log: True
wandb_project: NB-Haber-GPT-Training
wandb_run_name: haber-90k-gpt-v1.1
wandb_run_id: "1721342908"
dtype: float16
