
seq_length: 512
gradient_accumulation_steps: 1
batch_size: 32
max_iters : 1000
warmup_iters: 100
dtype: float32
compile: False
    
# learning rate
learning_rate:  0.0006
min_lr: 0.00006
decay_lr: True

# optimizer
weight_decay: 0.1
beta1:  0.9
beta2: 0.95

# logging
log_interval: 1
eval_interval: 1 # must be multiple of log interval
eval_iters: 1
promised_flops : 6.7e+12 # gtx 1070 on fp32
log_to:
#  - wandb
  - csv
wandb_project: MUP-GPT2-Small